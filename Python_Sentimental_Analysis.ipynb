{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx pandas scikit-elm statsmodels"
      ],
      "metadata": {
        "id": "2FJGBUmk9uKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import docx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import inv\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from docx import Document\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from docx import Document\n",
        "from numpy.linalg import inv"
      ],
      "metadata": {
        "id": "RzUJswRgERJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    data = []\n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            row_data = [cell.text for cell in row.cells]\n",
        "            data.append(row_data)\n",
        "    return data\n",
        "\n",
        "def write_data_to_csv(data, file_path):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(file_path, index=False, header=False)\n",
        "\n",
        "docx_file_path = 'Bostondata.docx'\n",
        "csv_file_path = 'Bostondata.csv'\n",
        "data = read_data_from_docx(docx_file_path)\n",
        "write_data_to_csv(data, csv_file_path)\n",
        "df = pd.read_csv(csv_file_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "P1VlRUpy3Orc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from docx import Document\n",
        "import time\n",
        "\n",
        "class MixedRegressionELM:\n",
        "    def __init__(self, hidden_units, activation_function='sigmoid', regularization=1e-5, random_type='normal'):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation_function = activation_function\n",
        "        self.regularization = regularization\n",
        "        self.random_type = random_type\n",
        "        self.bias = np.random.randn(hidden_units)  # Added\n",
        "        self.beta_hat_d = None\n",
        "        self.u_hat_d = None\n",
        "        self.best_d = None\n",
        "\n",
        "\n",
        "    def __input2hidden(self, X):\n",
        "        activations = {\n",
        "            'sigmoid': lambda x: 1 / (1 + np.exp(-(x + self.bias))),  # Added bias\n",
        "            'relu': lambda x: (x + self.bias) * (x > 0),               # Added bias\n",
        "            'sin': lambda x: np.sin(x + self.bias),                   # Added bias\n",
        "            'tanh': lambda x: np.tanh(x + self.bias),                 # Added bias\n",
        "            'leaky_relu': lambda x: np.maximum(0, x + self.bias) + 0.1 * np.minimum(0, x + self.bias)  # Add bias\n",
        "        }\n",
        "        return activations[self.activation_function](X)\n",
        "\n",
        "\n",
        "    def __hidden2output(self, H):\n",
        "        return np.dot(H, self.beta_hat_d)\n",
        "\n",
        "    def fit(self, H, y, d=0):\n",
        "        start_time = time.process_time()\n",
        "        Ip = np.eye(H.shape[1]) * self.regularization\n",
        "        HtH = np.dot(H.T, H) + Ip\n",
        "        Hty = np.dot(H.T, y)\n",
        "        self.beta_hat_d = np.linalg.solve(HtH + d * Ip, Hty)\n",
        "        train_time = time.process_time() - start_time\n",
        "\n",
        "        # Ensure beta_hat_d is reshaped to (n, 1)\n",
        "        self.beta_hat_d = self.beta_hat_d.reshape(-1, 1)  # Reshape to (n, 1) if necessary\n",
        "        return self.beta_hat_d, train_time\n",
        "\n",
        "    def fit_random_effects(self, Z, y, Hb_hat):\n",
        "        G_inv = np.linalg.inv(np.dot(Z.T, Z) + self.regularization * np.eye(Z.shape[1]))\n",
        "        HTG_invH = np.dot(Hb_hat.T, np.dot(G_inv, Hb_hat))\n",
        "        self.beta_hat = np.dot(np.linalg.inv(HTG_invH + self.regularization * np.eye(Hb_hat.shape[1])), np.dot(Hb_hat.T, np.dot(G_inv, y)))\n",
        "        A = HTG_invH + self.regularization * np.eye(Hb_hat.shape[1])\n",
        "        B_inv = np.linalg.inv(HTG_invH + d * np.eye(Hb_hat.shape[1]))\n",
        "        self.beta_hat_d = np.dot(B_inv, np.dot(A, self.beta_hat))\n",
        "        self.u_hat_d = np.dot(G_inv, np.dot(Z.T, (y - np.dot(Hb_hat, self.beta_hat_d))))\n",
        "\n",
        "    def predict(self, H, Z):\n",
        "        Hb_hat_d = np.dot(H, self.beta_hat_d)\n",
        "        if self.u_hat_d is None:\n",
        "            self.u_hat_d = np.zeros((Z.shape[1], 1))\n",
        "        Zu_hat_d = np.dot(Z, self.u_hat_d)\n",
        "        return Hb_hat_d + Zu_hat_d\n",
        "\n",
        "    def score(self, H, Z, y):\n",
        "        prediction = self.predict(H, Z)\n",
        "        return np.sqrt(mean_squared_error(y, prediction))\n",
        "\n",
        "    def count_accurate_predictions(self, H, Z, y, tolerance=0.1):\n",
        "        predictions = self.predict(H, Z)\n",
        "        accurate_predictions = np.abs(predictions - y) <= tolerance * y\n",
        "        num_accurate = np.sum(accurate_predictions)\n",
        "        return num_accurate, len(y), num_accurate / len(y)\n",
        "\n",
        "    def beta_hat_d(self, H, y, d):\n",
        "        Λ = np.dot(H.T, H)\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(Λ)\n",
        "        T = eigenvectors\n",
        "        Λ = eigenvalues\n",
        "        F_d = (Λ + d) / (Λ + 1)\n",
        "        gamma_hat = np.dot(T.T, self.beta_hat_d)\n",
        "        gamma_hat_d = F_d * gamma_hat\n",
        "        return np.dot(T, gamma_hat_d)\n",
        "\n",
        "    def beta_hat_k(self, H, y, d, k):\n",
        "        if self.beta_hat_d is None:\n",
        "            self.fit(H, y, d=d)\n",
        "\n",
        "        beta_d = self.beta_hat_d\n",
        "        if k < len(beta_d):\n",
        "            return beta_d[k]\n",
        "        else:\n",
        "            raise ValueError(f\"Index k={k} is out of bounds for beta_d with length {len(beta_d)}.\")\n",
        "\n",
        "def read_docx_table(file_path):\n",
        "    doc = Document(file_path)\n",
        "    data = []\n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            row_data = [cell.text.strip() for cell in row.cells]\n",
        "            data.append(row_data)\n",
        "    return data\n",
        "\n",
        "def calculate_d_mse(sigma_squared, lambda_values, gamma):\n",
        "    numerator = np.sum(gamma ** 2) - sigma_squared\n",
        "    denominator = np.sum(sigma_squared + lambda_values * gamma ** 2)\n",
        "    return np.clip(numerator / denominator, 0, 1) if denominator != 0 else 0\n",
        "\n",
        "def calculate_d_cl(sigma_squared, lambda_values, gamma):\n",
        "    d_cl = 1 - sigma_squared * (np.sum(1 / (lambda_values + 1)) / np.sum(lambda_values * gamma ** 2))\n",
        "    return np.clip(d_cl, 0, 1)\n",
        "\n",
        "def calculate_d_gcv(RSS_d, n, tr_Hd):\n",
        "    return RSS_d / (n - 1 - tr_Hd)\n",
        "\n",
        "def d(H, beta_hat, sigma2):\n",
        "    n = H.shape[0]\n",
        "    p = H.shape[1]\n",
        "    residuals = y - np.dot(H, beta_hat)\n",
        "    residuals_T = residuals.T\n",
        "    G_inv = np.linalg.inv(np.dot(H.T, H) + np.eye(H.shape[1]) * sigma2)\n",
        "    sigma2 = np.dot(residuals_T, np.dot(G_inv, residuals)) / (n - p)\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(np.dot(H.T, H))\n",
        "    alpha = np.dot(eigenvectors.T, beta_hat)\n",
        "    numerator = np.sum([1 / (eigenvalue + 1) for eigenvalue in eigenvalues])\n",
        "    denominator = np.sum([lambda_i * (alpha_i ** 2) / ((lambda_i + 1) ** 2) for lambda_i, alpha_i in zip(eigenvalues, alpha)])\n",
        "    return 1 - (sigma2 * numerator / denominator)\n",
        "\n",
        "def d1(H, beta_hat, sigma2):\n",
        "    n = H.shape[0]\n",
        "    p = H.shape[1]\n",
        "    residuals = y - np.dot(H, beta_hat)\n",
        "    residuals_T = residuals.T\n",
        "    G_inv = np.linalg.inv(np.dot(H.T, H) + np.eye(H.shape[1]) * sigma2)\n",
        "    sigma2 = np.dot(residuals_T, np.dot(G_inv, residuals)) / (n - p)\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(np.dot(H.T, H))\n",
        "    alpha = np.dot(eigenvectors.T, beta_hat)\n",
        "    numerator = np.sum([1 / (eigenvalue * (eigenvalue + 1)) for eigenvalue in eigenvalues])\n",
        "    denominator = np.sum([(alpha[i] ** 2) / ((eigenvalues[i] + 1) ** 2) for i in range(len(eigenvalues))])\n",
        "    return 1 - (sigma2 * numerator / denominator)\n",
        "\n",
        "# Main code\n",
        "file_path = 'Bostondata.docx'\n",
        "data = read_docx_table(file_path)\n",
        "\n",
        "df = pd.DataFrame(data[1:], columns=data[0])\n",
        "for col in df.columns:\n",
        "    try:\n",
        "        df[col] = pd.to_numeric(df[col])\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "df = df.drop(columns=['TOWN', 'Nunmber', 'LLSTAT'])\n",
        "column_names = ['CRIM', 'CHAS', 'AGE', 'RM2', 'NOX2', 'B1', 'LDIS', 'LMEDV']\n",
        "df.columns = column_names\n",
        "\n",
        "X = df.drop(columns=['LMEDV', 'NOX2']).values\n",
        "Z = df['NOX2'].values.reshape(-1, 1)\n",
        "y = df['LMEDV'].values.reshape(-1, 1)\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_Z = MinMaxScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "Z_scaled = scaler_Z.fit_transform(Z)\n",
        "\n",
        "X_train, X_test, y_train, y_test, Z_train, Z_test = train_test_split(X_scaled, y, Z_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "hidden_units = 700\n",
        "input_dim = X_train.shape[1]\n",
        "weights = np.random.randn(input_dim, hidden_units)\n",
        "\n",
        "hidden_activations_train = np.dot(X_train, weights)\n",
        "hidden_activations_test = np.dot(X_test, weights)\n",
        "\n",
        "elm = MixedRegressionELM(hidden_units=hidden_units, activation_function='sigmoid')\n",
        "hidden_activations_train = elm._MixedRegressionELM__input2hidden(hidden_activations_train)\n",
        "hidden_activations_test = elm._MixedRegressionELM__input2hidden(hidden_activations_test)\n",
        "\n",
        "beta_d, train_time = elm.fit(hidden_activations_train, y_train)\n",
        "print(f\"Train time: {train_time:.4f} seconds\")\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = elm.predict(hidden_activations_train, Z_train)\n",
        "y_test_pred = elm.predict(hidden_activations_test, Z_test)\n",
        "\n",
        "# Calculate RMSE for train and test\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "# Display RMSE results\n",
        "print(f\"RMSE: {train_rmse:.4f}\")\n",
        "\n",
        "# Separator\n",
        "print(\"*\" * 20)\n",
        "\n",
        "# Final Beta values\n",
        "print(\"Beta values:\", beta_d.flatten())\n"
      ],
      "metadata": {
        "id": "pfqnCUDDX8kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up values for k and d\n",
        "k_values = [10**-5, 10**-4, 10**-3, 10**-2, 0.02, 0.1, 0.2, 1, 1.5, 2, 3, 4, 5]\n",
        "d_values = [10**-5, 10**-4, 10**-3, 10**-2, 0.02, 0.1, 0.2, 1]\n",
        "\n",
        "# Initialize storage for beta_hat_K_d and u_hat_K_d values for each combination\n",
        "beta_hat_K_d_values = {}\n",
        "u_hat_K_d_values = {}\n",
        "\n",
        "# Print initial shapes for debugging\n",
        "print(\"Initial shapes:\")\n",
        "print(f\"Z_train shape: {Z_train.shape}\")\n",
        "print(f\"hidden_activations_train shape: {hidden_activations_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "\n",
        "for d in d_values:\n",
        "    beta_d, train_time = elm.fit(hidden_activations_train, y_train, d=d)\n",
        "    elm.beta_hat_d = beta_d\n",
        "\n",
        "    for k in k_values:\n",
        "        # Get dimensions\n",
        "        n_samples = Z_train.shape[0]\n",
        "        n_features_z = Z_train.shape[1]\n",
        "        n_hidden = hidden_activations_train.shape[1]\n",
        "\n",
        "        # Ensure y_train is 2D\n",
        "        if len(y_train.shape) == 1:\n",
        "            y_train_reshaped = y_train.reshape(-1, 1)\n",
        "        else:\n",
        "            y_train_reshaped = y_train\n",
        "\n",
        "        # Calculate necessary matrices\n",
        "        Z_T_Z = Z_train.T @ Z_train\n",
        "        Z_T_y = Z_train.T @ y_train_reshaped\n",
        "        Z_T_H = Z_train.T @ hidden_activations_train\n",
        "        H_T_H = hidden_activations_train.T @ hidden_activations_train\n",
        "\n",
        "        # Print shapes for debugging\n",
        "        print(\"\\nMatrix shapes after initial calculations:\")\n",
        "        print(f\"Z_T_Z shape: {Z_T_Z.shape}\")\n",
        "        print(f\"Z_T_y shape: {Z_T_y.shape}\")\n",
        "        print(f\"Z_T_H shape: {Z_T_H.shape}\")\n",
        "        print(f\"H_T_H shape: {H_T_H.shape}\")\n",
        "\n",
        "        # Calculate inverse matrices\n",
        "        G_inv = np.linalg.inv(Z_T_Z + elm.regularization * np.eye(n_features_z))\n",
        "        A = H_T_H + elm.regularization * np.eye(n_hidden)\n",
        "        c_inv = np.linalg.inv(H_T_H + d * np.eye(n_hidden))\n",
        "        B_inv = np.linalg.inv(H_T_H + k * np.eye(n_hidden))\n",
        "\n",
        "        # Calculate beta_hat_K_d step by step with intermediate shape checks\n",
        "        print(\"\\nIntermediate calculation shapes:\")\n",
        "\n",
        "        # Modified calculation sequence\n",
        "        temp1 = Z_train @ G_inv @ Z_train.T  # (404, 404)\n",
        "        temp2 = hidden_activations_train.T @ (np.eye(n_samples) - temp1)  # (10, 404)\n",
        "        temp3 = temp2 @ y_train_reshaped  # (10, 1)\n",
        "        temp4 = c_inv @ temp3  # (10, 1)\n",
        "        temp5 = A @ temp4  # (10, 1)\n",
        "        beta_hat_K_d = B_inv @ temp5  # (10, 1)\n",
        "\n",
        "        print(f\"temp1 shape: {temp1.shape}\")\n",
        "        print(f\"temp2 shape: {temp2.shape}\")\n",
        "        print(f\"temp3 shape: {temp3.shape}\")\n",
        "        print(f\"temp4 shape: {temp4.shape}\")\n",
        "        print(f\"temp5 shape: {temp5.shape}\")\n",
        "        print(f\"beta_hat_K_d shape: {beta_hat_K_d.shape}\")\n",
        "\n",
        "        beta_hat_K_d_values[(d, k)] = beta_hat_K_d\n",
        "\n",
        "        # Calculate u_hat_K_d\n",
        "        u_hat_K_d = G_inv @ (Z_T_y - Z_T_H @ beta_hat_K_d)\n",
        "        print(f\"u_hat_K_d shape: {u_hat_K_d.shape}\")\n",
        "\n",
        "        u_hat_K_d_values[(d, k)] = u_hat_K_d\n",
        "\n",
        "        # Define prediction function\n",
        "        def make_predict_function(beta_k_d, u_k_d):\n",
        "            def predict(H, Z):\n",
        "                return H @ beta_k_d + Z @ u_k_d\n",
        "            return predict\n",
        "\n",
        "        current_predict = make_predict_function(beta_hat_K_d, u_hat_K_d)\n",
        "\n",
        "        # Calculate predictions\n",
        "        y_train_pred = current_predict(hidden_activations_train, Z_train)\n",
        "        y_test_pred = current_predict(hidden_activations_test, Z_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_rmse = np.sqrt(np.mean((y_train_reshaped - y_train_pred) ** 2))\n",
        "        test_rmse = np.sqrt(np.mean((y_test.reshape(-1, 1) - y_test_pred) ** 2))\n",
        "\n",
        "        # Print final results\n",
        "        print(\"\\nResults:\")\n",
        "        print(f\"Parameters: d={d}, k={k}\")\n",
        "        print(f\"Training RMSE: {train_rmse:.4f}\")\n",
        "        print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "        print(\"*\" * 70)"
      ],
      "metadata": {
        "id": "iTzYtvAyV2Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_all = np.concatenate((y_train, y_test), axis=0)\n",
        "train_predictions = elm.predict(hidden_activations_train, Z_train)\n",
        "test_predictions = elm.predict(hidden_activations_test, Z_test)\n",
        "\n",
        "predictions_all = np.concatenate((train_predictions, test_predictions), axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(np.arange(len(y_all)), y_all, color='blue', label='Actual LMEDV', alpha=0.6)\n",
        "plt.plot(np.arange(len(predictions_all)), predictions_all, color='red', label='Model Predictions', linewidth=2)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('LMEDV')\n",
        "plt.title('Actual vs. Predicted (Mixed Model)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "mQWeBTclV3QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STDV Train Predictions"
      ],
      "metadata": {
        "id": "lQPEDLrjNlHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "print(\"Standard Deviation of train is % s \"\n",
        "                % (statistics.stdev(train_predictions.flatten())))"
      ],
      "metadata": {
        "id": "YCF4X1N2LMTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STDV Test Predictions"
      ],
      "metadata": {
        "id": "6lVZOqYfNskc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "# data - set\n",
        "sample = (test_predictions)\n",
        "\n",
        "# Prints standard deviation\n",
        "# xbar is set to default value of 1\n",
        "print(\"Standard Deviation of test is % s \"\n",
        "                % (statistics.stdev(sample.flatten())))"
      ],
      "metadata": {
        "id": "HsAjDnkwNgAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Regg"
      ],
      "metadata": {
        "id": "bWkgteFU4FP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import inv\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from docx import Document\n",
        "\n",
        "class SimpleLinearRegressionOld:\n",
        "    def __init__(self):\n",
        "        self.beta_hat = None\n",
        "        self.beta_hat_d = None\n",
        "        self.u_hat_d = None\n",
        "\n",
        "    def fit(self, X, y, Z, regularization, d):\n",
        "        n = Z.shape[0]\n",
        "        G_inv = inv(Z @ Z.T + regularization * np.eye(n))\n",
        "        XTG_inv_X = X.T @ G_inv @ X\n",
        "        XTG_inv_X_inv = inv(XTG_inv_X)\n",
        "        self.beta_hat = XTG_inv_X_inv @ X.T @ G_inv @ y\n",
        "\n",
        "        A = (XTG_inv_X + d * np.eye(X.shape[1]))\n",
        "        B_inv = inv(XTG_inv_X + np.eye(X.shape[1]))\n",
        "        ###c_inv=inv(XTG_inv_X + k * np.eye(X.shape[1]))\n",
        "        ###D=x @ G_inv @ y\n",
        "        self.beta_hat_d = B_inv @ A @ self.beta_hat\n",
        "        ###self.beta_hat_k_d = B_inv @ A @ c_inv @ D\n",
        "\n",
        "        # Calculate u_hat_d\n",
        "        self.u_hat_d = Z.T @ G_inv @ (y - X @ self.beta_hat)\n",
        "\n",
        "        ### Calculate u_hat_K_d\n",
        "        ###self.u_hat_K_d =  G_inv @ (Z.T @ y - Z.T @ X @ self.beta_hat_k_d)\n",
        "\n",
        "    def predict(self, X, Z):\n",
        "        Xbeta_hat_d = X @ self.beta_hat_d\n",
        "        Zu_hat_d = Z @ self.u_hat_d\n",
        "        return Xbeta_hat_d + Zu_hat_d\n",
        "\n",
        "        ###def predict(self, X, Z):\n",
        "        ###Xbeta_hat_K_d = X @ self.beta_hat_k_d\n",
        "        ###Zu_hat_K_d = Z @ self.u_hat_K_d\n",
        "        ###return Xbeta_hat_k_d + Zu_hat_K_d\n",
        "\n",
        "    def score(self, X, y, Z):\n",
        "        y_pred = self.predict(X, Z)\n",
        "        return np.sqrt(np.mean((y - y_pred) ** 2))\n",
        "\n",
        "# Function to read data from a Word document\n",
        "def read_docx_table(file_path):\n",
        "    doc = Document(file_path)\n",
        "    data = []\n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            data.append([cell.text.strip() for cell in row.cells])  # Strip to remove extra spaces\n",
        "    return data\n",
        "\n",
        "# Read data from Word file\n",
        "file_path = 'Bostondata.docx'\n",
        "data = read_docx_table(file_path)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data[1:], columns=data[0])\n",
        "for col in df.columns:\n",
        "    try:\n",
        "        df[col] = pd.to_numeric(df[col])\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(columns=['TOWN', 'Nunmber', 'LLSTAT'])\n",
        "column_names = ['CRIM', 'CHAS', 'AGE', 'RM2', 'NOX2', 'B1', 'LDIS', 'LMEDV']\n",
        "df.columns = column_names\n",
        "\n",
        "# Split data into X, Z, and y variables\n",
        "X = df.drop(columns=['LMEDV', 'NOX2']).values\n",
        "Z = df['NOX2'].values.reshape(-1, 1)\n",
        "y = df['LMEDV'].values.reshape(-1, 1)\n",
        "\n",
        "# Scale data\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_Z = MinMaxScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "Z_scaled = scaler_Z.fit_transform(Z)\n",
        "\n",
        "# Add noise to data\n",
        "np.random.seed(42)\n",
        "noise = np.random.normal(0, 0.5, size=X.shape)\n",
        "X_noisy = X + noise\n",
        "\n",
        "# Split data into training and test sets\n",
        "split_index = int(0.8 * len(X_noisy))\n",
        "X_train, X_test = X_noisy[:split_index], X_noisy[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Define ranges for k and d\n",
        "k_values = [10**(-i) for i in range(5, 1)] + [0.02, 0.1, 0.2, 1, 1.5, 2, 5]\n",
        "d_values = [10**(-i) for i in range(5, 1)] + [0.02, 0.1, 0.2, 1]\n",
        "\n",
        "# Create model and train it for each combination of k and d\n",
        "best_score = float('inf')\n",
        "best_d = None\n",
        "best_k = None\n",
        "best_model = None\n",
        "\n",
        "for d in d_values:\n",
        "    for k in k_values:\n",
        "        model = SimpleLinearRegressionOld()\n",
        "        regularization = k  # Use k as regularization parameter\n",
        "        model.fit(X_train, y_train, Z_scaled[:split_index], regularization, d)\n",
        "\n",
        "        # Calculate the score\n",
        "        train_score = model.score(X_train, y_train, Z_scaled[:split_index])\n",
        "        test_score = model.score(X_test, y_test, Z_scaled[split_index:])\n",
        "\n",
        "        # Print results for the current combination of d and k\n",
        "        print(f\"Testing d={d:.5f}, k={k:.5f} => Train RMSE: {train_score:.4f}, Test RMSE: {test_score:.4f}\")\n",
        "\n",
        "        # Save the best model based on test score\n",
        "        if test_score < best_score:\n",
        "            best_score = test_score\n",
        "            best_d = d\n",
        "            best_k = k\n",
        "            best_model = model\n",
        "\n",
        "# Print the best results\n",
        "print(\"-\" * 40)\n",
        "print(f\"Best Test RMSE: {best_score:.4f} for d={best_d:.5f}, k={best_k:.5f}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Plot comparison of predictions and actual values using the best model\n",
        "train_predictions = best_model.predict(X_train, Z_scaled[:split_index])\n",
        "test_predictions = best_model.predict(X_test, Z_scaled[split_index:])\n",
        "\n",
        "y_all = np.concatenate((y_train, y_test), axis=0)\n",
        "predictions_all = np.concatenate((train_predictions, test_predictions), axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(np.arange(len(y_all)), y_all, color='blue', label='Actual LMEDV')\n",
        "plt.plot(np.arange(len(predictions_all)), predictions_all, color='red', label='Model Predictions', linewidth=2)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('LMEDV')\n",
        "plt.title('Actual vs. Predicted (Simple Linear Regression)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cZC4rHVZ4I3e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}